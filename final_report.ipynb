{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective and Forecast "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\t- Business Problem: Predict a favorable (profitable) bet for the winner between two fighters at an UFC event. \n",
    "\t- ML Problem: Binary Classification (either one person wins or the other)\n",
    "\t- Metrics: Confusion matrix, Log Loss, and ROC-AUC curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Data and Sources "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a more detailed breakdown of the data, please scroll to the EDA section. This section will contain a quick preface of the structure of the data, and any outside sources to be credited. \n",
    "\n",
    "#### The Features \n",
    "\n",
    "The features contain a total of 118 columns that each fall in one of the following categories:\n",
    "\n",
    "\t- Identification variables: fighter names, dates, locations\n",
    "\t- Fighters' physical attributes: reach, height, weight, gender, weight class\n",
    "\t- Fighters' skill attributes: significant strike percentage, submissions and takedowns, win-loss record, win-loss streaks, ranking\n",
    "\t- Bout-specific details: total fight time, who won/how did they win (ect. Red - Unanimous Decision, or Blue - KO/TKO)\n",
    "\n",
    "#### The Target\n",
    "\n",
    "What we're trying to predict is the winner of each bout, presented in the train set as the 'Winner' column, with class levels being either 'Red' or 'Blue' (the colors of the corners in the UFC octagon - depending on which corner is whose, the fighter is either classified as a 'Red' fighter or 'Blue' fighter).\n",
    "\n",
    "Before training, I encoded these values as 0 and 1, with 'Red' being the former. Because there is a disproportionately large number of 'Red' winners, there were more 0's than 1's, and I had to be careful with the fact that features that seem less significant might be skewed towards the 0 side. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Note "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of the Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What I worked with - Insights from EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\t- Between the 118 numerical and categorical features, there was a high amount of dimensionality that had to be reduced. A lot of the columns were circumstantial (if one was nonzero, the others were not), and had a large amount of nulls \n",
    "\t- The dataset is imbalanced, erring in favor of fighters from the Red corner\n",
    "\t- A few of the continuous variables like height and reach are collinear, and so are a lot of the ordinal variables, creating a lot of noise  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What I tried - Approaches and Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried a total of 7 feature engineering approaches, and 3 total models.\n",
    "\n",
    "Feature engineering approaches: \n",
    "\t\n",
    "\t- Looking at all columns (ordinal, encoded categorical, and continuous) and using those that have the highest correlation with the target column\n",
    "\t- Finding a mix of ordinal and continous features that are skewed towards one outcome, or normaly distributed\n",
    "\t- Reducing dimensionality by consolidating variables unique to each fighter into one variable indexing the difference\n",
    "\n",
    "Models:\n",
    "\n",
    "\t- Ensemble methods like Random Forest, Gradient Boosting, etc. \n",
    "\t\t- Able to handle imbalanced data, and data that skews one way\n",
    "\t\t- Able to handle complex datasets with high dimensionality, robust to outliers \n",
    "\t\t- Particularly sensitive to features with more probabilistic pull on the outcome\n",
    "\t- Probabilistic models like Naive Bayes\n",
    "\t\t- A probabilistic estimate \n",
    "\t\t- Easily interpretable\n",
    "\t\t- Fast and useful for real-time prediciton\n",
    "\t- SVM\n",
    "\t\t- I thought there'd be a chance, if there was some sort of linear correlation, SVM could handle the high dimensionality through composition analysis\n",
    "\t\t- This ended up being a bust...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What worked and how I could tell - The best model, features, and hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best arrangement of features was actually a mix between the approaches. I picked a number of features that were normally distributed or heavily favored one outcome, and I also created a new variable called `perf_diff` which was the difference of each fighter's cumulative performance score (computed by aggregating their signifcant strike percentage, total takedown percentage, and total submissions attempted). \n",
    "\n",
    "The best model ended up being a random forest classifier - I figured this out by using four particular metrics:\n",
    "\n",
    "\t- Accuracy - across the board, the total number of correct predictions as a proportion of all. Worst is 0, best is 1. \n",
    "\t- Precision - interpreted using the confusion matrix. The number of true positive predictions as a proportion of all positive predictions. Worst is 0, best is 1.  \n",
    "\t- F1-score - a harmonic mean of precision and recall (true positive predictions as a proportion of ALL predictions). Worst is 0, best is 1. \n",
    "\t- Log Loss - a loss metric particularly effective for probabilistic outcomes. Best is 0, worst is 1. \n",
    "\n",
    "I used these metrics to discern the best hyperparameters for my particular instance of this algorithm. \n",
    "\n",
    "Finally, I used the trajectory of the ROC-AUC curve to set a probability threshold to fully maximize the potential of my model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA and Feature Engineering Concerns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from helpers import * \n",
    "import warnings\n",
    "from pandas.errors import SettingWithCopyWarning\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "ufc = pd.read_csv('ufc-master.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>R_fighter</th>\n",
       "      <th>B_fighter</th>\n",
       "      <th>R_odds</th>\n",
       "      <th>B_odds</th>\n",
       "      <th>R_ev</th>\n",
       "      <th>B_ev</th>\n",
       "      <th>date</th>\n",
       "      <th>location</th>\n",
       "      <th>country</th>\n",
       "      <th>Winner</th>\n",
       "      <th>...</th>\n",
       "      <th>finish_details</th>\n",
       "      <th>finish_round</th>\n",
       "      <th>finish_round_time</th>\n",
       "      <th>total_fight_time_secs</th>\n",
       "      <th>r_dec_odds</th>\n",
       "      <th>b_dec_odds</th>\n",
       "      <th>r_sub_odds</th>\n",
       "      <th>b_sub_odds</th>\n",
       "      <th>r_ko_odds</th>\n",
       "      <th>b_ko_odds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thiago Santos</td>\n",
       "      <td>Johnny Walker</td>\n",
       "      <td>-150.0</td>\n",
       "      <td>130</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>2021-10-02</td>\n",
       "      <td>Las Vegas, Nevada, USA</td>\n",
       "      <td>USA</td>\n",
       "      <td>Red</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5:00</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>800.0</td>\n",
       "      <td>900.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>1600.0</td>\n",
       "      <td>-110.0</td>\n",
       "      <td>175.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alex Oliveira</td>\n",
       "      <td>Niko Price</td>\n",
       "      <td>170.0</td>\n",
       "      <td>-200</td>\n",
       "      <td>170.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>2021-10-02</td>\n",
       "      <td>Las Vegas, Nevada, USA</td>\n",
       "      <td>USA</td>\n",
       "      <td>Blue</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5:00</td>\n",
       "      <td>900.0</td>\n",
       "      <td>450.0</td>\n",
       "      <td>350.0</td>\n",
       "      <td>700.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>550.0</td>\n",
       "      <td>120.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Misha Cirkunov</td>\n",
       "      <td>Krzysztof Jotko</td>\n",
       "      <td>110.0</td>\n",
       "      <td>-130</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>76.923077</td>\n",
       "      <td>2021-10-02</td>\n",
       "      <td>Las Vegas, Nevada, USA</td>\n",
       "      <td>USA</td>\n",
       "      <td>Blue</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5:00</td>\n",
       "      <td>900.0</td>\n",
       "      <td>550.0</td>\n",
       "      <td>275.0</td>\n",
       "      <td>275.0</td>\n",
       "      <td>1400.0</td>\n",
       "      <td>600.0</td>\n",
       "      <td>185.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alexander Hernandez</td>\n",
       "      <td>Mike Breeden</td>\n",
       "      <td>-675.0</td>\n",
       "      <td>475</td>\n",
       "      <td>14.814815</td>\n",
       "      <td>475.000000</td>\n",
       "      <td>2021-10-02</td>\n",
       "      <td>Las Vegas, Nevada, USA</td>\n",
       "      <td>USA</td>\n",
       "      <td>Red</td>\n",
       "      <td>...</td>\n",
       "      <td>Punch</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1:20</td>\n",
       "      <td>80.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>900.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>3500.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>1100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Joe Solecki</td>\n",
       "      <td>Jared Gordon</td>\n",
       "      <td>-135.0</td>\n",
       "      <td>115</td>\n",
       "      <td>74.074074</td>\n",
       "      <td>115.000000</td>\n",
       "      <td>2021-10-02</td>\n",
       "      <td>Las Vegas, Nevada, USA</td>\n",
       "      <td>USA</td>\n",
       "      <td>Blue</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5:00</td>\n",
       "      <td>900.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>400.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>900.0</td>\n",
       "      <td>600.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 119 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             R_fighter        B_fighter  R_odds  B_odds        R_ev  \\\n",
       "0        Thiago Santos    Johnny Walker  -150.0     130   66.666667   \n",
       "1        Alex Oliveira       Niko Price   170.0    -200  170.000000   \n",
       "2       Misha Cirkunov  Krzysztof Jotko   110.0    -130  110.000000   \n",
       "3  Alexander Hernandez     Mike Breeden  -675.0     475   14.814815   \n",
       "4          Joe Solecki     Jared Gordon  -135.0     115   74.074074   \n",
       "\n",
       "         B_ev        date                location country Winner  ...  \\\n",
       "0  130.000000  2021-10-02  Las Vegas, Nevada, USA     USA    Red  ...   \n",
       "1   50.000000  2021-10-02  Las Vegas, Nevada, USA     USA   Blue  ...   \n",
       "2   76.923077  2021-10-02  Las Vegas, Nevada, USA     USA   Blue  ...   \n",
       "3  475.000000  2021-10-02  Las Vegas, Nevada, USA     USA    Red  ...   \n",
       "4  115.000000  2021-10-02  Las Vegas, Nevada, USA     USA   Blue  ...   \n",
       "\n",
       "   finish_details finish_round finish_round_time  total_fight_time_secs  \\\n",
       "0             NaN          5.0              5:00                 1500.0   \n",
       "1             NaN          3.0              5:00                  900.0   \n",
       "2             NaN          3.0              5:00                  900.0   \n",
       "3           Punch          1.0              1:20                   80.0   \n",
       "4             NaN          3.0              5:00                  900.0   \n",
       "\n",
       "   r_dec_odds  b_dec_odds  r_sub_odds  b_sub_odds  r_ko_odds  b_ko_odds  \n",
       "0       800.0       900.0      2000.0      1600.0     -110.0      175.0  \n",
       "1       450.0       350.0       700.0      1100.0      550.0      120.0  \n",
       "2       550.0       275.0       275.0      1400.0      600.0      185.0  \n",
       "3       175.0       900.0       500.0      3500.0      110.0     1100.0  \n",
       "4       165.0       200.0       400.0      1200.0      900.0      600.0  \n",
       "\n",
       "[5 rows x 119 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ufc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4896, 119)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ufc.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Nulls "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially, I tried to handle nulls in a balanced way - initially by filling null values with aggregate statistics (the mean for a continuous feature's nulls, and the mode for categorical features). I also tried to proportion null values to better suit trends from that particular feature or other related features. Ultimately, however, I found it was best to simply drop the null values entirely, because it didn't affect any of the features' trends or omit a significant amount of information. \n",
    "\n",
    "I also found the rank variables useless because they "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_cols = []\n",
    "\n",
    "### Columns with more than half their values as nulls\n",
    "\n",
    "for col in ufc.columns:\n",
    "    if ufc[col].isna().sum() > ufc.shape[0] // 2:\n",
    "        bad_cols.append(col) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B_match_weightclass_rank',\n",
       " 'R_match_weightclass_rank',\n",
       " \"R_Women's Flyweight_rank\",\n",
       " \"R_Women's Featherweight_rank\",\n",
       " \"R_Women's Strawweight_rank\",\n",
       " \"R_Women's Bantamweight_rank\",\n",
       " 'R_Heavyweight_rank',\n",
       " 'R_Light Heavyweight_rank',\n",
       " 'R_Middleweight_rank',\n",
       " 'R_Welterweight_rank',\n",
       " 'R_Lightweight_rank',\n",
       " 'R_Featherweight_rank',\n",
       " 'R_Bantamweight_rank',\n",
       " 'R_Flyweight_rank',\n",
       " 'R_Pound-for-Pound_rank',\n",
       " \"B_Women's Flyweight_rank\",\n",
       " \"B_Women's Featherweight_rank\",\n",
       " \"B_Women's Strawweight_rank\",\n",
       " \"B_Women's Bantamweight_rank\",\n",
       " 'B_Heavyweight_rank',\n",
       " 'B_Light Heavyweight_rank',\n",
       " 'B_Middleweight_rank',\n",
       " 'B_Welterweight_rank',\n",
       " 'B_Lightweight_rank',\n",
       " 'B_Featherweight_rank',\n",
       " 'B_Bantamweight_rank',\n",
       " 'B_Flyweight_rank',\n",
       " 'B_Pound-for-Pound_rank',\n",
       " 'finish_details']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll drop these columns entirely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imbalanced Data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Winner\n",
       "Red     2859\n",
       "Blue    2037\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ufc.Winner.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will have to resample this with replacement. Some of the models we'll be looking at are able to handle imbalanced data, but it's definitely helpful to make sure the data is balanced for the sake of the model and for accurate insights from EDA. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic_ml_stuff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
