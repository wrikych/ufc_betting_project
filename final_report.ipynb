{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective and Forecast "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\t- Business Problem: Predict a favorable (profitable) bet for the winner between two fighters at an UFC event. \n",
    "\t- ML Problem: Binary Classification (either one person wins or the other)\n",
    "\t- Metrics: Confusion matrix, Log Loss, and ROC-AUC curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Data and Sources "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a more detailed breakdown of the data, please scroll to the EDA section. This section will contain a quick preface of the structure of the data, and any outside sources to be credited. \n",
    "\n",
    "#### The Features \n",
    "\n",
    "The features contain a total of 118 columns that each fall in one of the following categories:\n",
    "\n",
    "\t- Identification variables: fighter names, dates, locations\n",
    "\t- Fighters' physical attributes: reach, height, weight, gender, weight class\n",
    "\t- Fighters' skill attributes: significant strike percentage, submissions and takedowns, win-loss record, win-loss streaks, ranking\n",
    "\t- Bout-specific details: total fight time, who won/how did they win (ect. Red - Unanimous Decision, or Blue - KO/TKO)\n",
    "\n",
    "#### The Target\n",
    "\n",
    "What we're trying to predict is the winner of each bout, presented in the train set as the 'Winner' column, with class levels being either 'Red' or 'Blue' (the colors of the corners in the UFC octagon - depending on which corner is whose, the fighter is either classified as a 'Red' fighter or 'Blue' fighter).\n",
    "\n",
    "Before training, I encoded these values as 0 and 1, with 'Red' being the former. Because there is a disproportionately large number of 'Red' winners, there were more 0's than 1's, and I had to be careful with the fact that features that seem less significant might be skewed towards the 0 side. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Note "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of the Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What I worked with - Insights from EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\t- Between the 118 numerical and categorical features, there was a high amount of dimensionality that had to be reduced. A lot of the columns were circumstantial (if one was nonzero, the others were not), and had a large amount of nulls \n",
    "\t- The dataset is imbalanced, erring in favor of fighters from the Red corner\n",
    "\t- A few of the continuous variables like height and reach are collinear, and so are a lot of the ordinal variables, creating a lot of noise  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What I tried - Approaches and Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried a total of 7 feature engineering approaches, and 3 total models.\n",
    "\n",
    "Feature engineering approaches: \n",
    "\t\n",
    "\t- Looking at all columns (ordinal, encoded categorical, and continuous) and using those that have the highest correlation with the target column\n",
    "\t- Finding a mix of ordinal and continous features that are skewed towards one outcome, or normaly distributed\n",
    "\t- Reducing dimensionality by consolidating variables unique to each fighter into one variable indexing the difference\n",
    "\n",
    "Models:\n",
    "\n",
    "\t- Ensemble methods like Random Forest, Gradient Boosting, etc. \n",
    "\t\t- Able to handle imbalanced data, and data that skews one way\n",
    "\t\t- Able to handle complex datasets with high dimensionality, robust to outliers \n",
    "\t\t- Particularly sensitive to features with more probabilistic pull on the outcome\n",
    "\t- Probabilistic models like Naive Bayes\n",
    "\t\t- A probabilistic estimate \n",
    "\t\t- Easily interpretable\n",
    "\t\t- Fast and useful for real-time prediciton\n",
    "\t- SVM\n",
    "\t\t- I thought there'd be a chance, if there was some sort of linear correlation, SVM could handle the high dimensionality through composition analysis\n",
    "\t\t- This ended up being a bust...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What worked and how I could tell - The best model, features, and hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best arrangement of features was actually a mix between the approaches. I picked a number of features that were normally distributed or heavily favored one outcome, and I also created a new variable called `perf_diff` which was the difference of each fighter's cumulative performance score (computed by aggregating their signifcant strike percentage, total takedown percentage, and total submissions attempted). \n",
    "\n",
    "The best model ended up being a random forest classifier - I figured this out by using four particular metrics:\n",
    "\n",
    "\t- Accuracy - across the board, the total number of correct predictions as a proportion of all. Worst is 0, best is 1. \n",
    "\t- Precision - interpreted using the confusion matrix. The number of true positive predictions as a proportion of all positive predictions. Worst is 0, best is 1.  \n",
    "\t- F1-score - a harmonic mean of precision and recall (true positive predictions as a proportion of ALL predictions). Worst is 0, best is 1. \n",
    "\t- Log Loss - a loss metric particularly effective for probabilistic outcomes. Best is 0, worst is 1. \n",
    "\n",
    "I used these metrics to discern the best hyperparameters for my particular instance of this algorithm. \n",
    "\n",
    "Finally, I used the trajectory of the ROC-AUC curve to set a probability threshold to fully maximize the potential of my model. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
